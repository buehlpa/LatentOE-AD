{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['c:\\\\Users\\\\buehl\\\\Dropbox\\\\ZHAW\\\\MSE_DataScience\\\\23FS\\\\VT2\\\\Anomaly_detection_images\\\\bosch_AD\\\\LatentOE-AD\\\\Additional_Code_BULE', 'c:\\\\Users\\\\buehl\\\\git\\\\projects\\\\LatentOE-AD', 'c:\\\\Users\\\\buehl\\\\anaconda3\\\\envs\\\\AutencoderTF\\\\python39.zip', 'c:\\\\Users\\\\buehl\\\\anaconda3\\\\envs\\\\AutencoderTF\\\\DLLs', 'c:\\\\Users\\\\buehl\\\\anaconda3\\\\envs\\\\AutencoderTF\\\\lib', 'c:\\\\Users\\\\buehl\\\\anaconda3\\\\envs\\\\AutencoderTF', '', 'c:\\\\Users\\\\buehl\\\\anaconda3\\\\envs\\\\AutencoderTF\\\\lib\\\\site-packages', 'c:\\\\Users\\\\buehl\\\\anaconda3\\\\envs\\\\AutencoderTF\\\\lib\\\\site-packages\\\\win32', 'c:\\\\Users\\\\buehl\\\\anaconda3\\\\envs\\\\AutencoderTF\\\\lib\\\\site-packages\\\\win32\\\\lib', 'c:\\\\Users\\\\buehl\\\\anaconda3\\\\envs\\\\AutencoderTF\\\\lib\\\\site-packages\\\\Pythonwin']\n",
      "tensorflow_version:2.10.0\n",
      "torch_version:2.0.1\n"
     ]
    }
   ],
   "source": [
    "#activate conda env AutencoderTF env tf '2.10.0' , python 3.9.16\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "import torch\n",
    "import sys\n",
    "print(sys.path)\n",
    "sys.path.append('c:\\\\Users\\\\buehl\\\\Dropbox\\\\ZHAW\\\\MSE_DataScience\\\\23FS\\\\VT2\\\\Anomaly_detection_images\\\\bosch_AD\\\\LatentOE-AD')\n",
    "from loader.LoadData import CIFAR10_feat , FMNIST_feat\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score \n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import layers, losses\n",
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "\n",
    "print(f'tensorflow_version:{tf.__version__}')\n",
    "print(f'torch_version:{torch.__version__}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "(10000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
    "\n",
    "x_train = x_train.astype('float32') / 255.\n",
    "x_test = x_test.astype('float32') / 255.\n",
    "\n",
    "\n",
    "\n",
    "print (x_train.shape)\n",
    "print (x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 64 \n",
    "\n",
    "class Autoencoder(Model):\n",
    "  def __init__(self, latent_dim):\n",
    "    super(Autoencoder, self).__init__()\n",
    "    self.latent_dim = latent_dim   \n",
    "    self.encoder = tf.keras.Sequential([\n",
    "      layers.Flatten(),\n",
    "      layers.Dense(latent_dim, activation='relu'),\n",
    "    ])\n",
    "    self.decoder = tf.keras.Sequential([\n",
    "      layers.Dense(784, activation='sigmoid'),\n",
    "      layers.Reshape((28, 28))\n",
    "    ])\n",
    "\n",
    "  def call(self, x):\n",
    "    encoded = self.encoder(x)\n",
    "    decoded = self.decoder(encoded)\n",
    "    return decoded\n",
    "\n",
    "autoencoder = Autoencoder(latent_dim)\n",
    "autoencoder.compile(optimizer='adam', loss=losses.MeanSquaredError())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dot_img_file = 'C:/Users/buehl/OneDrive/Desktop/model_1.png'\n",
    "tf.keras.utils.plot_model(autoencoder, to_file=dot_img_file, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 10\n",
    "plt.figure(figsize=(20, 4))\n",
    "for i in range(n):\n",
    "  # display original\n",
    "  ax = plt.subplot(2, n, i + 1)\n",
    "  plt.imshow(x_test[test_mask][i])\n",
    "  plt.title(\"original\")\n",
    "  plt.gray()\n",
    "  ax.get_xaxis().set_visible(False)\n",
    "  ax.get_yaxis().set_visible(False)\n",
    "\n",
    "  # display reconstruction\n",
    "  ax = plt.subplot(2, n, i + 1 + n)\n",
    "  plt.imshow(decoded_imgs[i])\n",
    "  plt.title(\"reconstructed\")\n",
    "  plt.gray()\n",
    "  ax.get_xaxis().set_visible(False)\n",
    "  ax.get_yaxis().set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "188/188 [==============================] - 1s 2ms/step - loss: 0.0485\n",
      "Epoch 2/10\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0256\n",
      "Epoch 3/10\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0199\n",
      "Epoch 4/10\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0168\n",
      "Epoch 5/10\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0147\n",
      "Epoch 6/10\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0133\n",
      "Epoch 7/10\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0122\n",
      "Epoch 8/10\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0113\n",
      "Epoch 9/10\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0106\n",
      "Epoch 10/10\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0101\n",
      "Epoch 1/10\n",
      "188/188 [==============================] - 1s 3ms/step - loss: 0.0357\n",
      "Epoch 2/10\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0145\n",
      "Epoch 3/10\n",
      "188/188 [==============================] - 1s 3ms/step - loss: 0.0110\n",
      "Epoch 4/10\n",
      "188/188 [==============================] - 0s 3ms/step - loss: 0.0089\n",
      "Epoch 5/10\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0078\n",
      "Epoch 6/10\n",
      "188/188 [==============================] - 1s 3ms/step - loss: 0.0069\n",
      "Epoch 7/10\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0062\n",
      "Epoch 8/10\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0056\n",
      "Epoch 9/10\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0052\n",
      "Epoch 10/10\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0049\n",
      "Epoch 1/10\n",
      "188/188 [==============================] - 1s 2ms/step - loss: 0.0500\n",
      "Epoch 2/10\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0262\n",
      "Epoch 3/10\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0204\n",
      "Epoch 4/10\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0172\n",
      "Epoch 5/10\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0153\n",
      "Epoch 6/10\n",
      "188/188 [==============================] - 0s 1ms/step - loss: 0.0138\n",
      "Epoch 7/10\n",
      "188/188 [==============================] - 0s 1ms/step - loss: 0.0127\n",
      "Epoch 8/10\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0117\n",
      "Epoch 9/10\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0110\n",
      "Epoch 10/10\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0105\n",
      "Epoch 1/10\n",
      "188/188 [==============================] - 1s 2ms/step - loss: 0.0451\n",
      "Epoch 2/10\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0239\n",
      "Epoch 3/10\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0186\n",
      "Epoch 4/10\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0154\n",
      "Epoch 5/10\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0133\n",
      "Epoch 6/10\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0118\n",
      "Epoch 7/10\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0107\n",
      "Epoch 8/10\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0098\n",
      "Epoch 9/10\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0093\n",
      "Epoch 10/10\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0087\n",
      "Epoch 1/10\n",
      "188/188 [==============================] - 1s 2ms/step - loss: 0.0471\n",
      "Epoch 2/10\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0247\n",
      "Epoch 3/10\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0197\n",
      "Epoch 4/10\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0168\n",
      "Epoch 5/10\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0149\n",
      "Epoch 6/10\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0136\n",
      "Epoch 7/10\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0124\n",
      "Epoch 8/10\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0115\n",
      "Epoch 9/10\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0108\n",
      "Epoch 10/10\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0103\n",
      "Epoch 1/10\n",
      "188/188 [==============================] - 1s 2ms/step - loss: 0.0597\n",
      "Epoch 2/10\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0327\n",
      "Epoch 3/10\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0257\n",
      "Epoch 4/10\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0218\n",
      "Epoch 5/10\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0191\n",
      "Epoch 6/10\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0170\n",
      "Epoch 7/10\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0154\n",
      "Epoch 8/10\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0142\n",
      "Epoch 9/10\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0132\n",
      "Epoch 10/10\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0124\n",
      "Epoch 1/10\n",
      "188/188 [==============================] - 1s 2ms/step - loss: 0.0521\n",
      "Epoch 2/10\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0274\n",
      "Epoch 3/10\n",
      "188/188 [==============================] - 1s 3ms/step - loss: 0.0220\n",
      "Epoch 4/10\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0189\n",
      "Epoch 5/10\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0167\n",
      "Epoch 6/10\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0151\n",
      "Epoch 7/10\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0137\n",
      "Epoch 8/10\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0126\n",
      "Epoch 9/10\n",
      "188/188 [==============================] - 0s 1ms/step - loss: 0.0120\n",
      "Epoch 10/10\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0114\n",
      "Epoch 1/10\n",
      "188/188 [==============================] - 1s 1ms/step - loss: 0.0406\n",
      "Epoch 2/10\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0183\n",
      "Epoch 3/10\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0144\n",
      "Epoch 4/10\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0125\n",
      "Epoch 5/10\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0111\n",
      "Epoch 6/10\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0099\n",
      "Epoch 7/10\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0089\n",
      "Epoch 8/10\n",
      "188/188 [==============================] - 0s 1ms/step - loss: 0.0082\n",
      "Epoch 9/10\n",
      "188/188 [==============================] - 0s 1ms/step - loss: 0.0077\n",
      "Epoch 10/10\n",
      "188/188 [==============================] - 0s 1ms/step - loss: 0.0072\n",
      "Epoch 1/10\n",
      "188/188 [==============================] - 1s 2ms/step - loss: 0.0566\n",
      "Epoch 2/10\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0335\n",
      "Epoch 3/10\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0283\n",
      "Epoch 4/10\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0248\n",
      "Epoch 5/10\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0221\n",
      "Epoch 6/10\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0201\n",
      "Epoch 7/10\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0185\n",
      "Epoch 8/10\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0173\n",
      "Epoch 9/10\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0164\n",
      "Epoch 10/10\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0155\n",
      "Epoch 1/10\n",
      "188/188 [==============================] - 1s 2ms/step - loss: 0.0435\n",
      "Epoch 2/10\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0238\n",
      "Epoch 3/10\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0194\n",
      "Epoch 4/10\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0165\n",
      "Epoch 5/10\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0147\n",
      "Epoch 6/10\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0134\n",
      "Epoch 7/10\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0124\n",
      "Epoch 8/10\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0115\n",
      "Epoch 9/10\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0108\n",
      "Epoch 10/10\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0101\n"
     ]
    }
   ],
   "source": [
    "#np.random.seed(42)\n",
    "from skimage.metrics import mean_squared_error, normalized_root_mse,normalized_mutual_information\n",
    "\n",
    "pmi,mse,nrmse,label,anomaly=[],[],[],[],[]\n",
    "\n",
    "for i in list(set(y_train)):\n",
    "    autoencoder = Autoencoder(latent_dim)\n",
    "    autoencoder.compile(optimizer='adam', loss=losses.MeanSquaredError())\n",
    "    train_mask=(y_train==i)\n",
    "    autoencoder.fit(x_train[train_mask], x_train[train_mask],epochs=10,shuffle=True)\n",
    "    \n",
    "    #testing normal\n",
    "    test_mask=(y_test==i)\n",
    "    encoded_imgs = autoencoder.encoder(x_test[test_mask]).numpy()\n",
    "    decoded_imgs = autoencoder.decoder(encoded_imgs).numpy()\n",
    "\n",
    "    for j in range(0,len(encoded_imgs)):\n",
    "        mse.append(mean_squared_error(x_test[test_mask][j],decoded_imgs[j]))\n",
    "        nrmse.append(normalized_root_mse(x_test[test_mask][j],decoded_imgs[j]))\n",
    "        pmi.append(normalized_mutual_information(x_test[test_mask][j],decoded_imgs[j])-1)\n",
    "        label.append(i)\n",
    "        anomaly.append(0)\n",
    "    \n",
    "    #testing anomalies\n",
    "    test_mask_anomaly=(y_test!=i)\n",
    "    random_indexes = np.random.choice(range(0,len(x_test[test_mask_anomaly])), size=int(0.1*len(test_mask)), replace=False)\n",
    "    anomalies=x_test[test_mask_anomaly][random_indexes] \n",
    "\n",
    "    encoded_imgs_a = autoencoder.encoder(anomalies).numpy()\n",
    "    decoded_imgs_a = autoencoder.decoder(encoded_imgs_a).numpy()\n",
    "\n",
    "    for j in range(0,len(encoded_imgs_a)):\n",
    "        mse.append(mean_squared_error(anomalies[j],decoded_imgs_a[j]))\n",
    "        nrmse.append(normalized_root_mse(anomalies[j],decoded_imgs_a[j]))\n",
    "        pmi.append(normalized_mutual_information(anomalies[j],decoded_imgs_a[j])-1)\n",
    "        label.append(i)\n",
    "        anomaly.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "only_clean = pd.DataFrame({'mse_': mse, 'nrmse_': nrmse,'nmi_':pmi,'label_of_normal_': label,'anomaly_':anomaly})#, \n",
    "only_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.histplot(data=only_clean,x='label_of_normal_',y='mse_',hue='anomaly_').set(title=\"mse only clean data per class\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (20,20)\n",
    "sns.stripplot(data=only_clean,x='label_of_normal_',y='mse_',hue='anomaly_',palette=\"deep\").set(title=\"mse only clean data per class\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.metrics import mean_squared_error, normalized_root_mse,normalized_mutual_information\n",
    "from sklearn.preprocessing import normalize, MinMaxScaler\n",
    "root=\"C:/Users/buehl/Dropbox/ZHAW/MSE_DataScience/23FS/VT2/Anomaly_detection_images/bosch_AD/LatentOE-AD/DATA/fmnist_features/\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Vt2autoencoder",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
